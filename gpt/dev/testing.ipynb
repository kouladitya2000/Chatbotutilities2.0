{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code works on GPT-3.5 and embeddings model.\\n\\nThis Python script showcases a workflow for performing conversational retrieval tasks. It leverages Azure OpenAI's GPT-3.5 Turbo model and Azure Cognitive Search to seamlessly handle various steps in the process.\\n\\nFirst, it configures the OpenAI API and initializes essential components, such as language models and embeddings. It then efficiently loads, splits, and indexes text documents, making them easily searchable through Azure Cognitive Search.\\n\\nThis code serves as a practical example of how to integrate state-of-the-art natural language processing models and search capabilities to build powerful conversational retrieval systems, which can be invaluable for information retrieval and question-answering applications.\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This code works on GPT-3.5 and embeddings model.\n",
    "\n",
    "This Python script showcases a workflow for performing conversational retrieval tasks. It leverages Azure OpenAI's GPT-3.5 Turbo model and Azure Cognitive Search to seamlessly handle various steps in the process.\n",
    "\n",
    "First, it configures the OpenAI API and initializes essential components, such as language models and embeddings. It then efficiently loads, splits, and indexes text documents, making them easily searchable through Azure Cognitive Search.\n",
    "\n",
    "This code serves as a practical example of how to integrate state-of-the-art natural language processing models and search capabilities to build powerful conversational retrieval systems, which can be invaluable for information retrieval and question-answering applications.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import AzureSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\embeddings\\openai.py:214: UserWarning: WARNING! deployment_id is not default parameter.\n",
      "                    deployment_id was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_id is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('.env')\n",
    "\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "\n",
    "\n",
    "# Initialize gpt-35-turbo and our embedding model\n",
    "llm = AzureChatOpenAI(deployment_name=\"htiOaiDEP\")\n",
    "embeddings = OpenAIEmbeddings(deployment_id=\"htiOaiDEPte\", chunk_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure Cognitive Search and create index\n",
    "acs = AzureSearch(azure_search_endpoint=os.getenv('AZURE_COGNITIVE_SEARCH_SERVICE_NAME'),\n",
    "                 azure_search_key=os.getenv('AZURE_COGNITIVE_SEARCH_API_KEY'),\n",
    "                 index_name=os.getenv('AZURE_COGNITIVE_SEARCH_INDEX_NAME'),\n",
    "                 embedding_function=embeddings.embed_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NzI3MjBlMmEtMDk1ZC00NTliLTkzNDMtYmUwODhhYTQ2OGM4',\n",
       " 'YjVjMmFkMjEtZDJmOC00YWNmLTg5MDQtMjgwNmZiNjVkM2Rm',\n",
       " 'MzJiNjQ1ZTMtYzJlOC00NTM5LWE2ODAtYmQ0NDhjODAxOTYw']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"C:\\\\Users\\\\acer\\\\OneDrive\\\\Desktop\\\\update\\\\code\", glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})\n",
    "documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add documents to Azure Search\n",
    "acs.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=acs.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: who is Aditya Koul?\n",
      "Answer: I do not have enough information to answer that question. The context provided only states that Aditya Koul belongs to Dehradun, Uttarakhand, but it does not provide any further information about who they are.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"who is Aditya Koul?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
